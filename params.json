{"name":"SPLASH","tagline":"Parallel Stochastic Learning on Spark","body":"# Introduction\r\n\r\n**Stochastic algorithm** is a family of algorithms that perform iterative update by processing single data points. In each iteration, the algorithm randomly samples one element from the dataset, process it, and update an arbitrary set of shared variables. Typical examples of stochastic algorithm include **stochastic gradient descent**, **Gibbs sampling** and **stochastic variational inference**.\r\n\r\nSPLASH is a general-purpose tool for running stochastic algorithms on multi-node distributed systems. It provides a user-friendly interface for stochastic algorithm development. The main features of SPLASH are:\r\n\r\n* **Ease of Use**: Most stochastic algorithms can be immigrated to SPLASH for parallel execution without modification. The user writes code as if the program will be executed by a single-core processor. There is no need to devise a distributed implementation: the distributed implementation is automatically taken care of by the system. \r\n\r\n* **Fast Performance**: SPLASH workers won't synchronize until processing a large bulk of data. Thus, communication is not a bottleneck on the algorithm's efficiency. It makes SPLASH substantially faster than other parallel schemes which require frequent communication. SPLASH adopts novel strategies to resolve parallel processing conflicts, so that these conflicts don't hurt the algorithm's performance.\r\n\r\n* **Integration with Apache Spark**: SPLASH is built on [Apache Spark](https://spark.apache.org/). it takes the resilient distributed dataset (RDD) of Spark as input and generates RDD as output. In addition, it is seamlessly integrated with other data processing tools in the Spark environment, including the [MLlib machine learning library](https://spark.apache.org/mllib/). \r\n\r\n# Install SPLASH\r\n\r\nTo install SPLASH, you need to:\r\n\r\n1. Download and install [scala](http://www.scala-lang.org/index.html), [sbt](http://www.scala-sbt.org/index.html) and [Apache Spark](https://spark.apache.org/).\r\n2. Download the [SPLASH jar file](https://github.com/zhangyuc/splash/blob/gh-pages/target/scala-2.10/splash_2.10-0.0.1.jar?raw=true) and put it in your project classpath.\r\n3. Make SPLASH as a dependency when [submitting Spark jobs](http://spark.apache.org/docs/1.2.1/submitting-applications.html).\r\n\r\n# Quick Start\r\n\r\n## Import SPLASH\r\n\r\nWhen SPLASH is in your project classpath, you can write a self-contained application using the SPLASH API. Besides importing Spark packages, you also need to import the SPLASH package in scala, by typing:\r\n\r\n```scala\r\nimport splash.ParametrizedRDD\r\nimport splash.StreamProcessContext\r\nimport splash.ParameterSet\r\n```\r\n\r\n## Create Dataset\r\n\r\nJust as every Spark applications, the first step is to create a dataset for the learning algorithm. SPLASH provides an abstraction called **parametrized RDD**. The parametrized RDD is very similar to the resilient distributed dataset (RDD) of Spark, but it has particular data structure for maintaining the parameters to be learnt. A parametrized RDD can be created by a standard RDD:\r\n\r\n```scala\r\nval paramRdd = new ParametrizedRDD(data)\r\n```\r\n\r\nwhere `data` is a standard RDD object containing your dataset. \r\n\r\n## Define Stream Processing Function\r\n\r\nTo execute the learning algorithm on the dataset, set a stream processing function `update` to the parametrized RDD by\r\n\r\n```scala\r\nparamRdd.setProcessFunction(update)\r\n```\r\n\r\nThe `update` function is implemented by the user. It takes four objects as input: a random seed, a weighted data element in the dataset, the set of shared variables used by the stochastic algorithm and the set of local variables associated with the data element. An exemplary implementation of the **logistic regression** update is:\r\n\r\n```scala\r\nval update = (seed: Random, element: (Int, Array[String], Array[Double]), weight: Double, sharedVar: ParameterSet, localVar: ParameterSet ) => {\r\n  val y = element._1\r\n  val x_key = element._2\r\n  val x_value = element._3\r\n  val learningRate = 20.0\r\n  \r\n  // get the shared variable value by key \"t\"\r\n  val t = sharedVar.get(\"t\")\r\n  var sum = 0.0\r\n  for(i <- 0 until x_key.length){\r\n    // get the shared variable value by key \"w:\"+x_key(i)\r\n    sum += sharedVar.get(\"w:\"+x_key(i)) * x_value(i)\r\n  }\r\n  \r\n  for(i <- 0 until x_key.length)\r\n  {\r\n    val delta = weight * learningRate / math.sqrt( t + 1 ) * y  / (1.0 + math.exp(y*sum)) * x_value(i)\r\n                 \r\n    // increase the shared variable by delta\r\n    sharedVar.update(\"w:\" + x_key(i), delta)\r\n  }\r\n  // increase the shared variable by weight\r\n  sharedVar.update(\"t\", weight)\r\n}\r\n```\r\n\r\nin the above code `seed` is the random seed provided by the system. The data element `element` has three components: a binary label equal to -1 or 1, an array of feature indices and an array of feature values. The line of code\r\n\r\n```scala\r\nsum += sharedVar.get(\"w:\"+x_key(i)) * x_value(i)\r\n```\r\n\r\naccesses the weight associated with the feature index `x_key(i)` in order to compute a linear combination of feature values. Then the gradient of logistic loss with respect to this data element is computed, and stochastic gradient descent is applied to update the feature weights:\r\n\r\n```scala\r\nval delta = weight * learningRate / math.sqrt( t + 1 ) * y  / (1.0 + math.exp(y*sum)) * x_value(i)\r\nsharedVar.update(\"w:\" + x_key(i), delta)\r\n```\r\n\r\nThere is another shared variable `t` counting the number of elements that have been processed. It controls the stepsize of stochastic gradient descent. Similarly, it is manipulated by calling the `get` and `update` function on the shared variable set.\r\n\r\nMore generally, the value of local/shared variables are accessed by the `get` function by typing\r\n```scala\r\nval v1 = localVar.get(key)\r\nval v2 = sharedVar.get(key)\r\n```\r\nLocal variables are updated by directly putting a new value to the key:\r\n```scala\r\nlocalVar.set(key,value)\r\n```\r\nwhile shared variables are updated by putting the incremental change:\r\n```scala\r\nsharedVar.update(key,delta)\r\n```\r\nwhere `delta` is the difference between the new value and the old value. See the SPLASH API for more update options.\r\n\r\n## Stream Processing\r\n\r\nAfter setting up the stream processing function, the user calls `streamProcess` to process the dataset using the stream processing function:\r\n```scala\r\nval spc = (new StreamProcessContext).set(\"num.of.thread\", paramRdd.partitions.length)\r\nparamRdd.streamProcess(spc)\r\n```\r\nThe stream process context `spc` provides interfaces to control the learning process. The user can specify the number of parallel threads, the proportion of data to process per iteration, the strategy for combining parallel updates, etc. See the SPLASH API for more details. In this example, we adopt the system's default setting, but choosing the number of parallel threads to be equal to the number of RDD partitions. In the default setting, every element in the dataset is processed once by one call of `streamProcess`. In many machine learning applications, the user is recommended to call `streamProcess` multiple times to take multiple passes over the dataset, in order to obtain higher learning accuracy.\r\n\r\n## Output and Evaluation\r\n\r\nAfter processing the data, the shared variable set can be accessed by:\r\n```scala\r\nval sharedVar = paramRdd.getFirstSharedVariable()\r\n```\r\nwhich returns the shared variable set maintained by the first RDD partition. The user can also query the shared variable set from all RDD partitions by call `paramRdd.getAllSharedVariable()`. It is also possible to manipulate the parametrized RDD directly. For example, by calling\r\n```scala\r\nval loss = paramRdd.map(evaluateLoss).reduce( _ + _ )\r\n```\r\nevery element in the dataset is processed by the `evaluateLoss` function. The resulting losses are aggregated by the `reduce` operator. The `map` operator for parametrized RDD is different from the standard `map` in that it is granted access to not only a data element, but also the associated local and shared variables. As a concrete example, we define a function for evaluating the logistic loss:\r\n```scala\r\nval evaluateLoss = (element: (Int, Array[String], Array[Double]), sharedVar : ParameterSet,  localVar: ParameterSet ) => {\r\n  val y = element._1\r\n  val x_key = element._2\r\n  val x_value = element._3\r\n  \r\n  var sum = 0.0\r\n  for(i <- 0 until x_key.length){\r\n    sum += sharedVar.get(\"w:\" + x_key(i)) * x_value(i)\r\n  }\r\n  math.log( 1.0 + math.exp( - y * sum ) )\r\n}\r\n```\r\nIt provides a convenient way to evaluate the performance of the algorithm. See SPLASH API for more options of manipulating the parametrized RDD.\r\n\r\n## Try Logistic Regression Example\r\n\r\nTo run the logistic regression example (with stochastic gradient descent), download the [SPLASH example](https://github.com/zhangyuc/splash/blob/gh-pages/examples/SplashExample.tar.gz?raw=true) and extract it at any directory. The SPLASH library is included in the package, so you don't have to download it again. To run the code, `cd` into that directory, then compile the code by typing:\r\n```\r\nsbt package\r\n```\r\nIt generates a jar file at `./target/scala-2.10/simpleapp_2.10-1.0.jar`. We submit this jar file as a Spark job by typing\r\n```\r\nYOUR_SPARK_HOME/bin/spark-submit --class \"SimpleApp\" \\\r\n--driver-memory 4G \\\r\n--jars lib/splash_2.10-0.0.1.jar target/scala-2.10/simpleapp_2.10-1.0.jar \\\r\ndata/covtype.txt 20.0 > output.txt\r\n```\r\nHere, `SimpleApp` is the class name of the logistic regrsesion app. The file `splash_2.10-0.0.1.jar` is the SPLASH library and `simpleapp_2.10-1.0.jar` is the compiled code to be executed. The two arguments of the app are `data/covtype.txt` and `20.0`, which stand for the location of the data file and the learning rate. The result is output to `output.txt`.\r\n\r\nAfter the algorithm terminates (it takes 100 passes over the dataset), the output should be like:\r\n```\r\nStochastic Gradient Descent\r\nTime = 1.853; Loss = 0.52334904; Selected Sample Weight = 16.000000\r\nTime = 2.884; Loss = 0.51751886; Selected Sample Weight = 16.000000\r\nTime = 3.730; Loss = 0.51641545; Selected Sample Weight = 16.000000\r\nTime = 4.859; Loss = 0.51497963; Selected Sample Weight = 16.000000\r\nTime = 5.719; Loss = 0.51593768; Selected Sample Weight = 16.000000\r\n...\r\n```\r\nEach line corresponds to the outcome after an additional pass over the dataset. The three quantities are the total time elapsed, the average logistic loss and the adaptive sample weight chosen by the SPLASH system.\r\n\r\n# SPLASH API\r\n\r\nIn this section, we provide a brief description on the SPLASH API. \r\n\r\n## Parametrized RDD Operations\r\n\r\nThe parametrized RDD provides a similar set of operations that are supported by Spark RDD. Since the parametrized RDD maintains local variables and shared variables, there are additional operations manipulating these data structures.\r\n\r\n Operation | Meaning\r\n  --- | ---\r\nmap(*func*)       | Return a RDD formed by mapping each element by function `func`. The function takes the element and the associated local/shared variables as input\r\nforeach(*func*)       | Process each element by function `func`. The function takes the element and the associated local/shared variables as input.\r\nmapSharedVariable (*func*)  | Return a RDD formed by mapping the shared variable set by function `func`.\r\nforeachSharedVariable (*func*) | Process the shared variable set by function `func`.\r\nsyncSharedVariable() | Synchronize the shared variable of all partitions. This operation often follows the execution of the four operations above.\r\ngetFirstSharedVariable() | Return the set of shared variables in the first partition.\r\ngetAllSharedVariable() | Return an array of the set of shared variables in all partitions.\r\nsetProcessFunction (*func*) | Set the stream processing function. The function `func` takes a random seed, a weighted element and the associated local/shared variables. It performs update on the local/shared variables.\r\nsetPostProcessFunction (*func*) | Set the post-processing function. The function `func` takes a shared variable set after one iteration of stream processing, and performs post-processing on the shared variables.\r\nsetLossFunction(*func*) | Set a loss function for the stochastic algorithm. The function `func` takes an element and the associated local/shared variables. It returns the loss incurred by this element.\r\nstreamProcess(*spc*) | Use the stream processing function to process the dataset. `spc` is a StreamProcessContext object. It includes configurations for the stream processing procedure.\r\n\r\n## Parameter Set Operations\r\n\r\nBoth the local variables and the shared variables are organized by a ParameterSet instance. There are operations for reading and writing the parameter set.\r\n\r\n Operation | Meaning\r\n  --- | ---\r\nget(*key*) | Return the value of the key.\r\nset(*key*,*value*) | Put the value for the key.\r\nupdate (*key*,*delta*,*updateType*) | Update the value of the key by increasing it by `delta`. The `updateType` argument is optional. It has default value `Push` and alternative value `Keep`. If `updateType = Push`, then the update will eventually be pushed to all partitions. Otherwise, the updated is kept only by the current partition.\r\nupdateWithUnitWeight (*key*,*delta*,*updateType*) | The same functionality as `update`, but the system will assume that the current element has unit weight.\r\n\r\n## Stream Process Context\r\n\r\nThe Stream Process Context allows the user setting customized properties for the stream processing procedure. Given a Stream Process Context object `spc`, the properties are set by\r\n```scala\r\nspc = spc.set(propertyName,propertyValue)\r\n``` \r\nHere is a list of configurable properties:\r\n\r\nProperty Name | Default | Meaning\r\n--- | :---: | ---\r\nnum.of.thread | 1 | The number of parallel threads in stream processing.\r\ndata.per.iteration | 1.0 | Proportion of data processed per iteration.\r\napply.adaptive.reweighting | true | If the value is `true`, the system will adaptively choose element weights.\r\nreweight | 1.0 | If `apply.adaptive.reweighting = false`, then the element weights are chosen by `reweight`.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}